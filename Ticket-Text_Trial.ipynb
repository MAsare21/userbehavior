{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a311d45a-1816-408d-87a8-3b6415885244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#imports \n",
    "import json\n",
    "import collections\n",
    "import more_itertools\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import random\n",
    "import subprocess\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tqdm\n",
    "import csv\n",
    "import re\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "import mmap\n",
    "from tqdm import tqdm\n",
    "import openpyxl\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d3979a-3abd-4d92-8712-6916eecfdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: get_num_lines\n",
    "# Determines number of lines in a file to enable progress bars with tqdm\n",
    "\n",
    "def get_num_lines(filename):\n",
    "    fp = open(filename, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(),0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65004a56-2acd-441f-aeed-22cbeb8c6991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data -- import/upload xlsx spreadsheet\n",
    "data = '/pscratch/sd/m/masare/sacct/incident_sla.xlsx'\n",
    "#data = '/pscratch/sd/m/masare/sacct/incident_sla.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a57ad3d-21c5-4d4a-b0a8-d513a030eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_TXT_CSV = 'data_text_clean.csv'\n",
    "#SEARCH_TERM_LIST = 'software_exp_list.csv'\n",
    "SEARCH_TERM_LIST = 'Brandon_Software_Terms.csv'\n",
    "OUTPUT_FILE = 'AY24_Complete_BrandonTerms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8533b05f-90b1-41bb-8f41-b4c3f90d888f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21f06a7c-dc2e-4a27-a805-d2b662017f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "incident = pd.read_excel(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b416fa-6052-44a1-a95c-d3f35e807525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function: get_num_lines\n",
    "# Determines number of lines in a file to enable progress bars with tqdm\n",
    "\n",
    "def get_num_lines(filename):\n",
    "    fp = open(filename, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(),0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50abf552-396e-448b-bbaa-bb4c2ef02c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean \n",
    "   #isolate import columns, drop unimportant columns\n",
    "   #remove time stamps (can look at my code and use that) \n",
    "text_cols = [   'Number',\n",
    "                 'Created',\n",
    "                'Title',\n",
    "                'Additional comments',\n",
    "                'Close notes',\n",
    "                'Comments and Work notes',\n",
    "                'Staff work notes (NERSC private)']\n",
    "\n",
    "inc_text_df=incident[text_cols].copy()\n",
    "inc_text_df.fillna('', inplace=True)\n",
    "inc_text_df = inc_text_df.astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07790c9-d026-4ff2-aed6-afa4e6db2c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# :-----------------------------------------------------------------------------\n",
    "#clean \n",
    "   #isolate import columns, drop unimportant columns\n",
    "   #remove time stamps (can look at my code and use that) \n",
    "# Function: process_comment_text\n",
    "# Input: ind -- index from the pandas dataframe that holds the cleaned xlsx data\n",
    "# Output: Single row that includes, index, number, and text columns combined and\n",
    "#         cleaned into a single column entry.\n",
    "\n",
    "def process_comment_text(ind):\n",
    "    inc_text_entry = str( \\\n",
    "                     inc_text_df.iloc[ind][text_cols[0]] + ' ' + \\\n",
    "                     inc_text_df.iloc[ind][text_cols[1]] + ' ' + \\\n",
    "                     inc_text_df.iloc[ind][text_cols[2]] + ' ' + \\\n",
    "                     inc_text_df.iloc[ind][text_cols[3]] + ' ' + \\\n",
    "                     inc_text_df.iloc[ind][text_cols[4]] + ' ' + \\\n",
    "                     inc_text_df.iloc[ind][text_cols[5]])\n",
    "    text_data = inc_text_entry\n",
    "    text_data = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} - Guest \\(Additional comments\\)', '', text_data)\n",
    "    text_data = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} - System \\(Additional comments\\)', '', text_data)\n",
    "    text_data = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} - \\w+ \\w+ \\(Additional comments\\)', '', text_data)\n",
    "    text_data = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} - \\w+ \\w+ \\(Staff work notes \\(NERSC private\\)\\)', '', text_data)\n",
    "    text_data = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} - Guest \\(Staff work notes \\(NERSC private\\)\\)', '', text_data)\n",
    "    text_data = re.sub(r'@\\w+ \\w+', '', text_data)\n",
    "    text_data = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '',text_data)\n",
    "    #text_data = re.sub(r'\\d{2}:\\d{2}:\\d{2}', '', text_data) #remove system times\n",
    "    #text_data = re.sub(r'\\(Additional comments\\)', '', text_data)\n",
    "    #text_data = re.sub(r'\\(Staff work notes \\(NERSC private\\)\\)', '', text_data)\n",
    "    text_data = text_data.lower()\n",
    "    # remove html links\n",
    "    text_data = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '', text_data)\n",
    "    #text_data = re.sub(r'\\.','', text_data)\n",
    "    text_data = re.sub(r',','', text_data)\n",
    "    text_data = re.sub(r':','', text_data)\n",
    "    text_data = re.sub(r'\\?','', text_data)\n",
    "    # removing quotes can be problematic\n",
    "    #text_data = re.sub(r'\\\"','', text_data)\n",
    "    #text_data = re.sub(r'\\'','', text_data)\n",
    "    text_data = text_data.split()\n",
    "    wl = WordNetLemmatizer()\n",
    "    text_data = [wl.lemmatize(word) for word in text_data if not word in set(stopwords.words('english'))]\n",
    "    text_data = ' '.join(text_data)\n",
    "    return text_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc897bb-c007-4fce-a893-034dd0c0b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data set --> save as parquet \n",
    "CSV_OUTPUT_PATH = \"/pscratch/sd/m/masare/sacct/perlmutter\"\n",
    "STEPS_CSV_PATH = \"/pscratch/sd/m/masare/sacct/perlmutter/steps.csv\"\n",
    "JOBS_CSV_PATH = \"/pscratch/sd/m/masare/sacct/perlmutter/jobs.csv\"\n",
    "\n",
    "STEPS_PARQUET_PATH = STEPS_CSV_PATH.replace(\".csv\", \".parquet\")\n",
    "JOBS_PARQUET_PATH = JOBS_CSV_PATH.replace(\".csv\", \".parquet\")\n",
    "\n",
    "START_DATE = \"2023-01-01\"\n",
    "END_DATE = \"2023-07-31\"\n",
    "SACCT_DELIMITER = \"|||\"\n",
    "CSV_DELIMITER = \"\\t\"\n",
    "SACCT_FIELDS = [\n",
    "    'Number',\n",
    "    'Created',\n",
    "    'Title',\n",
    "    'Additional comments',\n",
    "    'Close notes',\n",
    "    'Comments and Work notes',\n",
    "    'Staff work notes (NERSC private)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2192942-ef94-4e70-b5a5-9fd1295743f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_windows(start, end, width=\"Created\"):\n",
    "    t0 = pd.Timestamp(start)\n",
    "    t1 = pd.Timestamp(end)\n",
    "    dt = pd.Timedelta(width)\n",
    "    starts = pd.date_range(t0, t1-dt, freq=width)\n",
    "    ends = starts + pd.Timedelta(width)\n",
    "    for s, e in zip(starts, ends):\n",
    "        yield s, e\n",
    "\n",
    "def sacct2csv(start, end, sacct_fields, csv_output_path, delimiter='|||', state=None, force=False):\n",
    "    \"\"\" Run `sacct` and save the output to csv\n",
    "    TODO: add --duplicates???\n",
    "    \"\"\"\n",
    "    SLURM_TIME_FORMAT=\"%s\"\n",
    "    cmd = [\n",
    "        \"sacct\",\n",
    "        \"--allusers\",\n",
    "        \"--parsable\",\n",
    "        \"--delimiter\", delimiter,\n",
    "        \"--format\", \",\".join(sacct_fields),\n",
    "        \"--starttime\", start,\n",
    "        \"--endtime\", end\n",
    "    ]\n",
    "    if state:\n",
    "        cmd += [\"--state\", f\"{state}\"]\n",
    "    \n",
    "    fout = f\"{csv_output_path}/{start}.raw.csv\"\n",
    "    if pathlib.Path(fout).exists() and not force:\n",
    "        return fout\n",
    "        \n",
    "    pathlib.Path(fout).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(fout, \"w\") as f:\n",
    "        subprocess.run(cmd, env={\"SLURM_TIME_FORMAT\":\"%s\"}, stdout=f)\n",
    "    return fout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f53a456a-6621-4822-a62f-24b217f76902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unit abbreviation w/o a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m days \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtime_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTART_DATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(days):\n\u001b[1;32m      3\u001b[0m     f \u001b[38;5;241m=\u001b[39m sacct2csv(start\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), end\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m                   SACCT_FIELDS, CSV_OUTPUT_PATH, SACCT_DELIMITER)\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mtime_windows\u001b[0;34m(start, end, width)\u001b[0m\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(start)\n\u001b[1;32m      3\u001b[0m t1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(end)\n\u001b[0;32m----> 4\u001b[0m dt \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m starts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(t0, t1\u001b[38;5;241m-\u001b[39mdt, freq\u001b[38;5;241m=\u001b[39mwidth)\n\u001b[1;32m      6\u001b[0m ends \u001b[38;5;241m=\u001b[39m starts \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(width)\n",
      "File \u001b[0;32mtimedeltas.pyx:1820\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timedeltas.Timedelta.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtimedeltas.pyx:653\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timedeltas.parse_timedelta_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unit abbreviation w/o a number"
     ]
    }
   ],
   "source": [
    "days = list(time_windows(START_DATE, END_DATE))\n",
    "for start, end in tqdm.tqdm(days):\n",
    "    f = sacct2csv(start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"),\n",
    "                  SACCT_FIELDS, CSV_OUTPUT_PATH, SACCT_DELIMITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce4281-e0ae-42b5-8bf1-931395efccaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flat_rows(p_in, delimiter):\n",
    "    \"\"\"Loop through rows\n",
    "    compress the extra whitespace\n",
    "    yield rows\"\"\"\n",
    "    re_whitespace = re.compile(br\"\\s+\")\n",
    "    row_delimiter = (delimiter + '\\n').encode()\n",
    "\n",
    "    with open(p_in, \"rb\") as f_in:\n",
    "        csv = f_in.read()\n",
    "        start, end = 0, 0\n",
    "        \n",
    "        # skip the 1st row\n",
    "        end = csv.find(row_delimiter, start)\n",
    "        start = end + len(row_delimiter)\n",
    "        \n",
    "        while end != -1:\n",
    "            end = csv.find(row_delimiter, start)\n",
    "            if end != -1:\n",
    "                row = re_whitespace.sub(b' ', csv[start:end]).strip()\n",
    "                yield(row + b'\\n')\n",
    "            start = end + len(row_delimiter)\n",
    "\n",
    "\n",
    "def decode_utf8(row):\n",
    "    \"\"\"Need replace here as users occassionally use weird chars\n",
    "    e.g. `sacct -j 5697497`\"\"\"\n",
    "    return row.decode(\"utf-8\", \"replace\")\n",
    "\n",
    "\n",
    "def decode_step_to_int(step):\n",
    "    \"\"\"Use the 32 bit int representation (lifted from Slurm source)\n",
    "    for the 'special' steps\"\"\"\n",
    "    \n",
    "    if step == 'extern':\n",
    "        return 0xFFFFFFFC\n",
    "    elif step == 'batch':\n",
    "        return 0xFFFFFFFB\n",
    "    elif step == 'interactive':\n",
    "        return 0xFFFFFFFA\n",
    "    else:\n",
    "        return int(step)\n",
    "    \n",
    "def decode_step_to_str(step):\n",
    "    if step == 'extern':\n",
    "        return '4294967292' # 0xFFFFFFFC\n",
    "    elif step == 'batch':\n",
    "        return '4294967291' # 0xFFFFFFFB\n",
    "    elif step == 'interactive':\n",
    "        return '4294967290' # 0xFFFFFFFA\n",
    "    else:\n",
    "        if step.isnumeric():\n",
    "            return step\n",
    "        else:\n",
    "            raise ValueError(f\"{step} not numeric\")\n",
    "\n",
    "def unpack_jobidraw(jobidraw):\n",
    "    jobstep = -1\n",
    "    hetjoboffset = 0\n",
    "    \n",
    "    fields = jobidraw.replace(\"+\", \".\").split(\".\")\n",
    "    n = len(fields)\n",
    "\n",
    "    hetjoboffset = \"0\"\n",
    "    if n == 1:\n",
    "        jobid = fields[0]\n",
    "        jobstep = -1\n",
    "    elif n == 2:\n",
    "        jobid = fields[0]\n",
    "        jobstep = decode_step_to_str(fields[1])\n",
    "    elif n == 3:\n",
    "        jobid = fields[0]\n",
    "        jobstep = decode_step_to_str(fields[1])\n",
    "        hetjoboffset = fields[2]\n",
    "    \n",
    "    return jobid, jobstep, hetjoboffset\n",
    "\n",
    "\n",
    "def transform_step_row(cols, delimiter):\n",
    "    \"\"\"\n",
    "    Could be <jobid> for an allocation\n",
    "    Generally <jobid>.<stepid>+<HetJobOffset> for steps\n",
    "        e.g. 13943741.2+1 \n",
    "    \"\"\"\n",
    "    \n",
    "    jobid, jobstep, hetjoboffset = unpack_jobidraw(cols[0])\n",
    "    row = delimiter.join(\n",
    "        [jobid, jobstep, hetjoboffset] + cols[1:])\n",
    "    return row\n",
    "\n",
    "\n",
    "def split_steps_and_jobs(row, delimiter, f_steps, f_jobs, d_out='\\t'):\n",
    "    cols = row.split(delimiter)\n",
    "    jobidraw = cols[0]\n",
    "    if '.' in jobidraw:\n",
    "        f_steps.write(transform_step_row(cols, d_out))\n",
    "    else:\n",
    "        f_jobs.write(d_out.join(cols))\n",
    "\n",
    "\n",
    "def write_header(f_in, delimiter, f_steps, f_jobs, d_out='\\t'):\n",
    "    header = f_in.readline().rstrip(delimiter + \"\\n\").replace(delimiter, d_out)\n",
    "    header = header.replace('Number', 'JobID')\n",
    "    f_jobs.write(header + \"\\n\")\n",
    "    header_steps = d_out.join(\n",
    "        [\"JobID\", \"JobStep\", \"HetJobOffset\"] + header.split(d_out)[1:]\n",
    "    )\n",
    "    f_steps.write(header_steps + \"\\n\")\n",
    "\n",
    "    \n",
    "def check_field_count(row, delimiter, n):\n",
    "    if len(row.split(delimiter)) != n:\n",
    "        print(\"WARN:\", row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9dfb2e-e600-4823-9246-bdbacf7fe2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_csvs = list(pathlib.Path(CSV_OUTPUT_PATH).glob(\"*.raw.csv\"))\n",
    "\n",
    "with (open(STEPS_CSV_PATH, \"w\") as f_steps,\n",
    "      open(JOBS_CSV_PATH, \"w\") as f_jobs):\n",
    "    \n",
    "    with open(raw_csvs[0], \"r\") as csv0:\n",
    "        write_header(csv0, SACCT_DELIMITER, f_steps, f_jobs, CSV_DELIMITER)\n",
    "    \n",
    "    for p_in in tqdm.tqdm(raw_csvs):\n",
    "        for row in (decode_utf8(r) for r in flat_rows(p_in, SACCT_DELIMITER)):\n",
    "            split_steps_and_jobs(row, SACCT_DELIMITER, f_steps, f_jobs, CSV_DELIMITER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8743d-9c16-459c-b837-4504400fcda2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = collections.defaultdict(lambda: str)\n",
    "dtypes[\"JobID\"] = np.int32\n",
    "dtypes[\"Created\"] = np.int32\n",
    "dtypes[\"Title\"] = str #np.int32\n",
    "dtypes[\"Additional comments\"] = str #np.int64\n",
    "dtypes[\"Close notes\"] = str #np.int16\n",
    "dtypes[\"Comments and Work notes\"] = str #np.int32\n",
    "dtypes[\"Staff work notes (NERSC private)\"] = str  #np.int64\n",
    "#dtypes[\"Submit\"] = np.int64\n",
    "dtypes[\"Start\"] = str # np.int64, str b/c it may not have started\n",
    "dtypes[\"End\"] = str # np.int64, str b/c it may not have started\n",
    "#dtypes[\"ElapsedEaw\"] = np.int64\n",
    "#dtypes['HetJobOffset'] = np.int8\n",
    "\n",
    "# Some columns are just always empty for steps so no need to keep those\n",
    "# They will need to be joined back in from the jobs table\n",
    "step_cols = [\n",
    "    'JobID',\n",
    "    'Created',\n",
    "    'Title',\n",
    "    'Additional comments',\n",
    "    'Close notes',\n",
    "    'Comments and Work notes',\n",
    "    'Staff work notes (NERSC private)'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce247c7a-f657-438a-a2aa-cdccf4698e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    STEPS_CSV_PATH,\n",
    "    delimiter = CSV_DELIMITER,\n",
    "    on_bad_lines = 'warn',\n",
    "    dtype = dtypes,\n",
    "    usecols = step_cols, \n",
    "    memory_map = True)\n",
    "df.to_parquet(STEPS_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80f521-871f-4a23-b433-20112cf1f071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe742d-d46b-4e2a-8eb7-9f648ee42aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sacct",
   "language": "python",
   "name": "sacct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
