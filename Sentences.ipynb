{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beeeb60-7558-423a-b525-a40af238343c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from unidecode import unidecode\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.sklearn\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab9977-b11f-4c99-9018-863840703ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/global/homes/m/masare/ticket-text-analysis/ticket_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f212c7-ebdb-4a6b-9aee-dc70a9e2e6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = df['text_data']\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055710e-7126-49e6-b148-73cdebca7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8efbb-3a71-4f9b-96ff-d055253d9869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380898a5-232a-4340-8346-dd873ed68c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_val = df.text_data.apply(unidecode).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505091f-e3a6-47f6-94e0-c1ab36906054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00aa3c-11a4-4201-9a7f-a7871ef8eb1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The sentences to encode\n",
    "#sentences = [\n",
    "   # \"The weather is lovely today.\",\n",
    "  #  \"It's so sunny outside!\",\n",
    " #   \"He drove to the stadium.\",\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227906ac-2af3-4a71-9ad5-fa1f0f0331ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(df_val, show_progress_bar=True)\n",
    "print(embeddings.shape)\n",
    "# [6944, 384]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac4c4f-bd56-4264-a536-32814b619a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aed7b0-947c-4ae5-9458-d6f441a0badb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform kmean clustering\n",
    "num_clusters = 30\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(similarities)\n",
    "cluster_assignment = clustering_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e423f65-21c3-4616-ac2e-0ad999165493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(df_val[sentence_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655376a4-0847-49b2-864f-bb54bb3ded84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for i, cluster in enumerate(clustered_sentences):\n",
    "   # print(\"Cluster \", i + 1)\n",
    "  #  print(cluster)\n",
    "  #  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635a0eb-8806-4306-b2e7-e5ac15e3e59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print a limited number of sentences per cluster\n",
    "#for i, cluster in enumerate(clustered_sentences):\n",
    " #   print(f\"Cluster {i + 1}\")\n",
    " #   print(cluster[:5])  # Print only the first 5 sentences in each cluster\n",
    "  #  print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccdaaa-a4f5-471c-ad0a-91ad9224afb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# To see the total number of sentences in each cluster\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(f\"Cluster {i + 1} has {len(cluster)} sentences\")\n",
    "    print(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d18dd-6e93-4865-b5ae-df6f61286ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = df_val.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846461e-a33b-47d0-8304-41189a9a0049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c934083-1be0-4d10-8b86-0d5298f25997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "preprocessed_data = [preprocess(text) for text in text_data]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c29da2-5e8a-41cf-96f4-5096b53eb308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "text_vectorized = vectorizer.fit_transform(preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178759d3-3057-4cfa-880d-9f345273140e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply LDA for topic modeling\n",
    "num_topics = 30  # Number of topics\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(text_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2c510-4b22-43cf-8ab2-0d09471ac613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the topics\n",
    "def get_topics(model, vectorizer, top_n=10):\n",
    "    topics = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        topics[f'Topic {idx}'] = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "    return topics\n",
    "\n",
    "\n",
    "topics = get_topics(lda_model, vectorizer)\n",
    "for topic, words in topics.items():\n",
    "    print(f\"{topic}: {', '.join(words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c7473-e233-4f1c-b84b-2f1d0e4e80e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0890b-27a4-4f5c-bbd9-c71b683d3a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the pyLDAvis visualization\n",
    "#import pyLDAvis.sklearn\n",
    "import pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_vis_data = pyLDAvis.lda_model.prepare(lda_model, text_vectorized, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5fbd7f-a1e2-4525-b513-7e2416f83813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the visualization\n",
    "pyLDAvis.display(lda_vis_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3dbc39-c0f7-45af-843f-d149f922fbf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Convert the prepared data to JSON\n",
    "lda_vis_json = pyLDAvis.save_json(lda_vis_data)\n",
    "\n",
    "\n",
    "# Load the JSON data\n",
    "lda_vis_dict = json.loads(lda_vis_json)\n",
    "\n",
    "\n",
    "# Customize the topic colors\n",
    "topic_colors = [\"#FF0000\", \"#00FF00\", \"#0000FF\", \"#FF00FF\", \"#00FFFF\", \"#FFFF00\", \"#FF8000\", \"#8000FF\", \"#00FF80\", \"#80FF00\"]\n",
    "for i, color in enumerate(topic_colors):\n",
    "    lda_vis_dict['topic']['colors'][str(i+1)] = color\n",
    "\n",
    "\n",
    "# Save the modified JSON data back\n",
    "with open(\"lda_vis_custom.json\", \"w\") as f:\n",
    "    json.dump(lda_vis_dict, f)\n",
    "\n",
    "\n",
    "# Load the modified JSON data and display the visualization\n",
    "pyLDAvis.display(pyLDAvis.display_data(lda_vis_dict))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3b4b8-844e-4481-b566-3499212e1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semantic Textual Similarity\n",
    "# Two lists of sentences\n",
    "sentences1 = [\n",
    "    \"Scratch purge extension\",\n",
    "    \"new allocation year\",\n",
    "    \"python not working\",\n",
    "    \"I like cheese\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a88b2-40ac-4ad5-b50f-857372562d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences2 = [\n",
    "    \"authentication\",\n",
    "    \"no username\",\n",
    "    \"mfa not working\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098469a4-562d-4938-b90a-23614b1ec9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences2 = [\n",
    "    \"my python notebook won't run\",\n",
    "    \"new account\",\n",
    "    \"mfa not working\",\n",
    "    \"I hate cheese\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2b45f-7d37-48be-bcdb-693ce6e34d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute embeddings for both lists\n",
    "embeddings1 = model.encode(sentences1)\n",
    "embeddings2 = model.encode(sentences2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b4aa2-9019-4a2c-b5dd-214712fb12d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute cosine similarities\n",
    "similarities2 = model.similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a7fdd-9509-449e-a2e0-2855e01aca8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output the pairs with their score\n",
    "for idx_i, sentence1 in enumerate(sentences1):\n",
    "    print(sentence1)\n",
    "    for idx_j, sentence2 in enumerate(sentences2):\n",
    "        print(f\" - {sentence2: <30}: {similarities2[idx_i][idx_j]:.4f}\")\n",
    "#he SentenceTransformer.similarity method returns a 3x3 matrix with the respective cosine similarity scores for all possible pairs between embeddings1 and embeddings2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921f029-744d-42ad-8111-d025ea68a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####             BERTopic\n",
    "#Install these packages in terminal\n",
    "#pip install bertopic\n",
    "\n",
    "# Choose an embedding backend\n",
    "#pip install bertopic[flair,gensim,spacy,use]\n",
    "\n",
    "# Topic modeling with images\n",
    "#pip install bertopic[vision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66974743-422a-4f83-b136-a4ce092bd443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = df['text_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27994ae6-5775-4a78-bbe3-10d04f26b490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#text_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638db9e5-b567-4629-8479-1ecc59f5732e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sentence Splitter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = [sent_tokenize(text_data) for text_data in text_data]\n",
    "sentences = [sentence for doc in sentences for sentence in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7138a-b015-4582-94eb-e97f63975204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a72f20-54b3-408a-8308-660a61a11a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Controlling Number of Topics\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47362c2-9db2-4f18-b952-a41892d102c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Improving Default Representation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe294a1-1e0c-48a9-a827-63f99e131a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3fc7e-d999-4736-af83-9e0388583722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Additional Representations\n",
    "import openai\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
    "\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# Part-of-Speech\n",
    "pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
    "\n",
    "# MMR\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# GPT-3.5\n",
    "prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
    "topic: <topic label>\n",
    "\"\"\"\n",
    "client = openai.OpenAI(api_key=\"sk-...\")\n",
    "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    # \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n",
    "    \"MMR\": mmr_model,\n",
    "    \"POS\": pos_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b74fd-9b9f-416f-add9-87224fb01328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sacct",
   "language": "python",
   "name": "sacct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
